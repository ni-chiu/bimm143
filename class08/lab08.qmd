---
title: "Mini-Project: Unsupervised Learning Analysis of Human Breast Cancer Cells"
author: "Nicholas Chiu"
Date: "2024-02-11"
format: markdown_github
---

## 1. EDA

```{r}
## Data Prep
fna.data <- "WisconsinCancer.csv"

## Input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

#wisc.df
diagnosis <- factor(wisc.df[,1]) 

wisc.data <- wisc.df[,-1]

## EDA
#wisc.data

malignant <- diagnosis[diagnosis == "M"]
length(malignant)

columns <- colnames(wisc.data)
columns
grep("_mean", columns)

```
Q1: There are 569 observations in the dataset

Q2: There are 212 observations with a malignant diagnosis

Q3: There are 10 columns (variables) with the suffix "_mean"

## PCA

```{r}
## Performing PCA
colMeans(wisc.data)

apply(wisc.data,2,sd)

wisc.pr <- prcomp(wisc.data, scale=TRUE)

summary(wisc.pr)

```
Q4: 44.27%

Q5: 3 (PC1, PC2, PC3 is 73% of variation)

Q6: 7 (PC1:7 is 91% of variation)

```{r}
## Interpreting PCA results

biplot(wisc.pr)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1:2], col = diagnosis, xlab = "PC1", ylab = "PC2")

# Scatter plot observations by components 1 and 3
plot(wisc.pr$x[,c(1,3)], col = diagnosis, xlab = "PC1", ylab = "PC3")
```
Q7: There are many vectors pointing in the same direction. The plot
is very difficult to understand because there are too many variables and observations
that make the plot very messy.

Q8: Both plots have 2 distinct subgroups but plot 2 has more overlap between the 
subgroups compared to plot 1. 

```{r}
## ggplot
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
## Variance

pr.var <- wisc.pr$sdev^2
head(pr.var)

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

```{r}
## Communicating PCA Results

wisc.pr$rotation["concave.points_mean",1]

head(pve)
sum(pve[1:5])
```


Q9: -0.261

Q10: 5 PCs

## 3. Hierarchical Clustering

```{r}
## Data manipulation

data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method="complete")

plot(wisc.hclust)
abline(h=19, col="red", lty=2)

```
Q11: height = 19

```{r}
## Selecting number of clusters

wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
plot(wisc.hclust.clusters)

table(wisc.hclust.clusters, diagnosis)

wisc.hclust.clust <- cutree(wisc.hclust,k=8)
table(wisc.hclust.clust, diagnosis)

```
Q12: Using 8 clusters appears to produce a better cluster vs. diagnoses
match because there are more distinct clusters that that correspond to diagnoses.

```{r}
## Other methods
wisc.hclust <- hclust(data.dist, method="ward.D2")
plot(wisc.hclust)

wisc.hclust.c <- cutree(wisc.hclust,k=4)
plot(wisc.hclust.c)

table(wisc.hclust.c, diagnosis)

```
Q13: The "ward.D2" method produces my favorite results because the plot produces 
clear cluster levels and the table produces the most number of clusters that 
correspond to one of the diagnoses.

## 4. K-means Clustering -- Optional (skipped)


## 5. Combining Methods

```{r}
## Clustering
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")

plot(wisc.pr.hclust)

grps <- cutree(wisc.pr.hclust, k=2)
table(grps)

table(grps, diagnosis)

plot(wisc.pr$x[,1:2], col=grps)

plot(wisc.pr$x[,1:2], col=diagnosis)

g <- as.factor(grps)
levels(g)

g <- relevel(g,2)
levels(g)

plot(wisc.pr$x[,1:2], col=g)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

table(wisc.pr.hclust.clusters, diagnosis)

#wisc.hclust.clusters2 <- cutree(wisc.hclust, k=2)

table(wisc.hclust.clusters, diagnosis)
```
Q15: It does well

Q16: The other clustering models do well in separating the clusters as well 
but yield slightly different results from the new model.

## 6. Sensitivity/Specificity

```{r}
## Sensitivity
newmodelSen <- 188/(188+28)
kmeansSen <- 175/(175+14)
hclustSen <- 165/(165+12)

newmodelSen
kmeansSen
hclustSen

## Specificity
newmodelSpec <- 329/(329+24)
kmeansSpec <- 343/(343+37)
hclustSpec <- 343/(343+40)

newmodelSpec
kmeansSpec
hclustSpec
```
Q17: The original hclust model does best for sensitivity and the new hclust model
does best for specificity.

## 7. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc

plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
Q18: Based on the results, we should prioritize patient 2. 


